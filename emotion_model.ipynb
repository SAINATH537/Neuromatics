{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68ca24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5746c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e47e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (48, 48)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "TRAIN_DIR = \"C:/MENATL_HEALTH/train\"  \n",
    "TEST_DIR = \"C:/MENATL_HEALTH/test\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b57494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a2f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bba705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c306e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.0266046844269623, 1: 9.406618610747051, 2: 1.0010460615781582, 3: 0.5684387684387684, 4: 0.8260394187886635, 5: 0.8491274770777877, 6: 1.293372978330405}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f9817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">687,079</span> (2.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m687,079\u001b[0m (2.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">684,647</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m684,647\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> (9.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,432\u001b[0m (9.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(input_shape=(48, 48, 1), num_classes=7):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d94c067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, monitor='val_loss', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "    ModelCheckpoint('best_emotion_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2122e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32, tf.float32), but the yielded element was (array([[[[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6745098 ],\n         [0.6784314 ],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6784314 ],\n         [0.67058825],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6666667 ],\n         [0.65882355],\n         [0.6745098 ]],\n\n        ...,\n\n        [[0.43921572],\n         [0.4431373 ],\n         [0.46274513],\n         ...,\n         [0.8117648 ],\n         [0.8196079 ],\n         [0.8117648 ]],\n\n        [[0.43921572],\n         [0.45098042],\n         [0.45098042],\n         ...,\n         [0.80392164],\n         [0.8078432 ],\n         [0.81568635]],\n\n        [[0.4431373 ],\n         [0.46274513],\n         [0.43921572],\n         ...,\n         [0.78823537],\n         [0.79215693],\n         [0.7960785 ]]],\n\n\n       [[[0.29803923],\n         [0.29803923],\n         [0.3019608 ],\n         ...,\n         [0.854902  ],\n         [0.83921576],\n         [0.82745105]],\n\n        [[0.32156864],\n         [0.30588236],\n         [0.29803923],\n         ...,\n         [0.85098046],\n         [0.8352942 ],\n         [0.8196079 ]],\n\n        [[0.34901962],\n         [0.34901962],\n         [0.34509805],\n         ...,\n         [0.8431373 ],\n         [0.82745105],\n         [0.81568635]],\n\n        ...,\n\n        [[0.3137255 ],\n         [0.18431373],\n         [0.13725491],\n         ...,\n         [0.09411766],\n         [0.10196079],\n         [0.19607845]],\n\n        [[0.25490198],\n         [0.16470589],\n         [0.13333334],\n         ...,\n         [0.12156864],\n         [0.13333334],\n         [0.30980393]],\n\n        [[0.18823531],\n         [0.14117648],\n         [0.14117648],\n         ...,\n         [0.15294118],\n         [0.23137257],\n         [0.38431376]]],\n\n\n       [[[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        ...,\n\n        [[0.22352943],\n         [0.19607845],\n         [0.3137255 ],\n         ...,\n         [0.28235295],\n         [0.28235295],\n         [0.28235295]],\n\n        [[0.21960786],\n         [0.20784315],\n         [0.20784315],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]],\n\n        [[0.20784315],\n         [0.227451  ],\n         [0.19215688],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]]],\n\n\n       ...,\n\n\n       [[[0.35686275],\n         [0.35686275],\n         [0.35686275],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.35686275],\n         [0.35686275],\n         [0.36078432],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.36078432],\n         [0.35686275],\n         [0.3529412 ],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        ...,\n\n        [[0.54509807],\n         [0.4666667 ],\n         [0.5058824 ],\n         ...,\n         [0.8352942 ],\n         [0.82745105],\n         [0.81568635]],\n\n        [[0.5254902 ],\n         [0.4784314 ],\n         [0.5176471 ],\n         ...,\n         [0.8196079 ],\n         [0.82745105],\n         [0.8352942 ]],\n\n        [[0.5019608 ],\n         [0.49411768],\n         [0.5176471 ],\n         ...,\n         [0.8078432 ],\n         [0.8078432 ],\n         [0.8078432 ]]],\n\n\n       [[[0.01568628],\n         [0.01568628],\n         [0.03137255],\n         ...,\n         [0.16862746],\n         [0.0627451 ],\n         [0.03529412]],\n\n        [[0.01568628],\n         [0.01960784],\n         [0.03137255],\n         ...,\n         [0.13333334],\n         [0.04313726],\n         [0.04313726]],\n\n        [[0.01568628],\n         [0.02352941],\n         [0.03137255],\n         ...,\n         [0.1137255 ],\n         [0.03137255],\n         [0.0509804 ]],\n\n        ...,\n\n        [[0.47450984],\n         [0.49411768],\n         [0.49411768],\n         ...,\n         [0.06666667],\n         [0.06666667],\n         [0.07058824]],\n\n        [[0.4901961 ],\n         [0.5137255 ],\n         [0.5372549 ],\n         ...,\n         [0.02745098],\n         [0.03137255],\n         [0.03529412]],\n\n        [[0.5254902 ],\n         [0.5411765 ],\n         [0.56078434],\n         ...,\n         [0.00392157],\n         [0.00392157],\n         [0.00392157]]],\n\n\n       [[[0.6117647 ],\n         [0.6117647 ],\n         [0.40784317],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        [[0.60784316],\n         [0.61960787],\n         [0.47058827],\n         ...,\n         [0.654902  ],\n         [0.654902  ],\n         [0.654902  ]],\n\n        [[0.6       ],\n         [0.5921569 ],\n         [0.4039216 ],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        ...,\n\n        [[0.7411765 ],\n         [0.6392157 ],\n         [0.6117647 ],\n         ...,\n         [0.4431373 ],\n         [0.45098042],\n         [0.45098042]],\n\n        [[0.74509805],\n         [0.6392157 ],\n         [0.63529414],\n         ...,\n         [0.5529412 ],\n         [0.5529412 ],\n         [0.5529412 ]],\n\n        [[0.7568628 ],\n         [0.6627451 ],\n         [0.6627451 ],\n         ...,\n         [0.6431373 ],\n         [0.6431373 ],\n         [0.6509804 ]]]], dtype=float32), array([0., 3., 3., 4., 6., 6., 0., 5., 5., 2., 4., 2., 4., 0., 4., 6., 4.,\n       3., 5., 0., 3., 0., 0., 5., 2., 6., 6., 5., 2., 3., 3., 2., 5., 3.,\n       3., 3., 3., 0., 4., 3., 6., 5., 0., 3., 2., 4., 2., 3., 4., 2., 0.,\n       3., 6., 0., 4., 0., 3., 6., 6., 3., 3., 0., 3., 3.], dtype=float32)).\nTraceback (most recent call last):\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 204, in generator_py_func\n    flattened_values = nest.flatten_up_to(output_types, values)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 237, in flatten_up_to\n    return nest_util.flatten_up_to(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1541, in flatten_up_to\n    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1570, in _tf_data_flatten_up_to\n    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1427, in _tf_data_assert_shallow_structure\n    raise ValueError(\n\nValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 206, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32, tf.float32), but the yielded element was (array([[[[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6745098 ],\n         [0.6784314 ],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6784314 ],\n         [0.67058825],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6666667 ],\n         [0.65882355],\n         [0.6745098 ]],\n\n        ...,\n\n        [[0.43921572],\n         [0.4431373 ],\n         [0.46274513],\n         ...,\n         [0.8117648 ],\n         [0.8196079 ],\n         [0.8117648 ]],\n\n        [[0.43921572],\n         [0.45098042],\n         [0.45098042],\n         ...,\n         [0.80392164],\n         [0.8078432 ],\n         [0.81568635]],\n\n        [[0.4431373 ],\n         [0.46274513],\n         [0.43921572],\n         ...,\n         [0.78823537],\n         [0.79215693],\n         [0.7960785 ]]],\n\n\n       [[[0.29803923],\n         [0.29803923],\n         [0.3019608 ],\n         ...,\n         [0.854902  ],\n         [0.83921576],\n         [0.82745105]],\n\n        [[0.32156864],\n         [0.30588236],\n         [0.29803923],\n         ...,\n         [0.85098046],\n         [0.8352942 ],\n         [0.8196079 ]],\n\n        [[0.34901962],\n         [0.34901962],\n         [0.34509805],\n         ...,\n         [0.8431373 ],\n         [0.82745105],\n         [0.81568635]],\n\n        ...,\n\n        [[0.3137255 ],\n         [0.18431373],\n         [0.13725491],\n         ...,\n         [0.09411766],\n         [0.10196079],\n         [0.19607845]],\n\n        [[0.25490198],\n         [0.16470589],\n         [0.13333334],\n         ...,\n         [0.12156864],\n         [0.13333334],\n         [0.30980393]],\n\n        [[0.18823531],\n         [0.14117648],\n         [0.14117648],\n         ...,\n         [0.15294118],\n         [0.23137257],\n         [0.38431376]]],\n\n\n       [[[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        ...,\n\n        [[0.22352943],\n         [0.19607845],\n         [0.3137255 ],\n         ...,\n         [0.28235295],\n         [0.28235295],\n         [0.28235295]],\n\n        [[0.21960786],\n         [0.20784315],\n         [0.20784315],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]],\n\n        [[0.20784315],\n         [0.227451  ],\n         [0.19215688],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]]],\n\n\n       ...,\n\n\n       [[[0.35686275],\n         [0.35686275],\n         [0.35686275],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.35686275],\n         [0.35686275],\n         [0.36078432],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.36078432],\n         [0.35686275],\n         [0.3529412 ],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        ...,\n\n        [[0.54509807],\n         [0.4666667 ],\n         [0.5058824 ],\n         ...,\n         [0.8352942 ],\n         [0.82745105],\n         [0.81568635]],\n\n        [[0.5254902 ],\n         [0.4784314 ],\n         [0.5176471 ],\n         ...,\n         [0.8196079 ],\n         [0.82745105],\n         [0.8352942 ]],\n\n        [[0.5019608 ],\n         [0.49411768],\n         [0.5176471 ],\n         ...,\n         [0.8078432 ],\n         [0.8078432 ],\n         [0.8078432 ]]],\n\n\n       [[[0.01568628],\n         [0.01568628],\n         [0.03137255],\n         ...,\n         [0.16862746],\n         [0.0627451 ],\n         [0.03529412]],\n\n        [[0.01568628],\n         [0.01960784],\n         [0.03137255],\n         ...,\n         [0.13333334],\n         [0.04313726],\n         [0.04313726]],\n\n        [[0.01568628],\n         [0.02352941],\n         [0.03137255],\n         ...,\n         [0.1137255 ],\n         [0.03137255],\n         [0.0509804 ]],\n\n        ...,\n\n        [[0.47450984],\n         [0.49411768],\n         [0.49411768],\n         ...,\n         [0.06666667],\n         [0.06666667],\n         [0.07058824]],\n\n        [[0.4901961 ],\n         [0.5137255 ],\n         [0.5372549 ],\n         ...,\n         [0.02745098],\n         [0.03137255],\n         [0.03529412]],\n\n        [[0.5254902 ],\n         [0.5411765 ],\n         [0.56078434],\n         ...,\n         [0.00392157],\n         [0.00392157],\n         [0.00392157]]],\n\n\n       [[[0.6117647 ],\n         [0.6117647 ],\n         [0.40784317],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        [[0.60784316],\n         [0.61960787],\n         [0.47058827],\n         ...,\n         [0.654902  ],\n         [0.654902  ],\n         [0.654902  ]],\n\n        [[0.6       ],\n         [0.5921569 ],\n         [0.4039216 ],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        ...,\n\n        [[0.7411765 ],\n         [0.6392157 ],\n         [0.6117647 ],\n         ...,\n         [0.4431373 ],\n         [0.45098042],\n         [0.45098042]],\n\n        [[0.74509805],\n         [0.6392157 ],\n         [0.63529414],\n         ...,\n         [0.5529412 ],\n         [0.5529412 ],\n         [0.5529412 ]],\n\n        [[0.7568628 ],\n         [0.6627451 ],\n         [0.6627451 ],\n         ...,\n         [0.6431373 ],\n         [0.6431373 ],\n         [0.6509804 ]]]], dtype=float32), array([0., 3., 3., 4., 6., 6., 0., 5., 5., 2., 4., 2., 4., 0., 4., 6., 4.,\n       3., 5., 0., 3., 0., 0., 5., 2., 6., 6., 5., 2., 3., 3., 2., 5., 3.,\n       3., 3., 3., 0., 4., 3., 6., 5., 0., 3., 2., 4., 2., 3., 4., 2., 0.,\n       3., 6., 0., 4., 0., 3., 6., 6., 3., 3., 0., 3., 3.], dtype=float32)).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_7341]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     train_generator,\n\u001b[0;32m      3\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE,\n\u001b[0;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_generator,\n\u001b[0;32m      5\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mtest_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH_SIZE,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m      8\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weights\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\choud\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32, tf.float32), but the yielded element was (array([[[[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6745098 ],\n         [0.6784314 ],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6784314 ],\n         [0.67058825],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6666667 ],\n         [0.65882355],\n         [0.6745098 ]],\n\n        ...,\n\n        [[0.43921572],\n         [0.4431373 ],\n         [0.46274513],\n         ...,\n         [0.8117648 ],\n         [0.8196079 ],\n         [0.8117648 ]],\n\n        [[0.43921572],\n         [0.45098042],\n         [0.45098042],\n         ...,\n         [0.80392164],\n         [0.8078432 ],\n         [0.81568635]],\n\n        [[0.4431373 ],\n         [0.46274513],\n         [0.43921572],\n         ...,\n         [0.78823537],\n         [0.79215693],\n         [0.7960785 ]]],\n\n\n       [[[0.29803923],\n         [0.29803923],\n         [0.3019608 ],\n         ...,\n         [0.854902  ],\n         [0.83921576],\n         [0.82745105]],\n\n        [[0.32156864],\n         [0.30588236],\n         [0.29803923],\n         ...,\n         [0.85098046],\n         [0.8352942 ],\n         [0.8196079 ]],\n\n        [[0.34901962],\n         [0.34901962],\n         [0.34509805],\n         ...,\n         [0.8431373 ],\n         [0.82745105],\n         [0.81568635]],\n\n        ...,\n\n        [[0.3137255 ],\n         [0.18431373],\n         [0.13725491],\n         ...,\n         [0.09411766],\n         [0.10196079],\n         [0.19607845]],\n\n        [[0.25490198],\n         [0.16470589],\n         [0.13333334],\n         ...,\n         [0.12156864],\n         [0.13333334],\n         [0.30980393]],\n\n        [[0.18823531],\n         [0.14117648],\n         [0.14117648],\n         ...,\n         [0.15294118],\n         [0.23137257],\n         [0.38431376]]],\n\n\n       [[[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        ...,\n\n        [[0.22352943],\n         [0.19607845],\n         [0.3137255 ],\n         ...,\n         [0.28235295],\n         [0.28235295],\n         [0.28235295]],\n\n        [[0.21960786],\n         [0.20784315],\n         [0.20784315],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]],\n\n        [[0.20784315],\n         [0.227451  ],\n         [0.19215688],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]]],\n\n\n       ...,\n\n\n       [[[0.35686275],\n         [0.35686275],\n         [0.35686275],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.35686275],\n         [0.35686275],\n         [0.36078432],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.36078432],\n         [0.35686275],\n         [0.3529412 ],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        ...,\n\n        [[0.54509807],\n         [0.4666667 ],\n         [0.5058824 ],\n         ...,\n         [0.8352942 ],\n         [0.82745105],\n         [0.81568635]],\n\n        [[0.5254902 ],\n         [0.4784314 ],\n         [0.5176471 ],\n         ...,\n         [0.8196079 ],\n         [0.82745105],\n         [0.8352942 ]],\n\n        [[0.5019608 ],\n         [0.49411768],\n         [0.5176471 ],\n         ...,\n         [0.8078432 ],\n         [0.8078432 ],\n         [0.8078432 ]]],\n\n\n       [[[0.01568628],\n         [0.01568628],\n         [0.03137255],\n         ...,\n         [0.16862746],\n         [0.0627451 ],\n         [0.03529412]],\n\n        [[0.01568628],\n         [0.01960784],\n         [0.03137255],\n         ...,\n         [0.13333334],\n         [0.04313726],\n         [0.04313726]],\n\n        [[0.01568628],\n         [0.02352941],\n         [0.03137255],\n         ...,\n         [0.1137255 ],\n         [0.03137255],\n         [0.0509804 ]],\n\n        ...,\n\n        [[0.47450984],\n         [0.49411768],\n         [0.49411768],\n         ...,\n         [0.06666667],\n         [0.06666667],\n         [0.07058824]],\n\n        [[0.4901961 ],\n         [0.5137255 ],\n         [0.5372549 ],\n         ...,\n         [0.02745098],\n         [0.03137255],\n         [0.03529412]],\n\n        [[0.5254902 ],\n         [0.5411765 ],\n         [0.56078434],\n         ...,\n         [0.00392157],\n         [0.00392157],\n         [0.00392157]]],\n\n\n       [[[0.6117647 ],\n         [0.6117647 ],\n         [0.40784317],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        [[0.60784316],\n         [0.61960787],\n         [0.47058827],\n         ...,\n         [0.654902  ],\n         [0.654902  ],\n         [0.654902  ]],\n\n        [[0.6       ],\n         [0.5921569 ],\n         [0.4039216 ],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        ...,\n\n        [[0.7411765 ],\n         [0.6392157 ],\n         [0.6117647 ],\n         ...,\n         [0.4431373 ],\n         [0.45098042],\n         [0.45098042]],\n\n        [[0.74509805],\n         [0.6392157 ],\n         [0.63529414],\n         ...,\n         [0.5529412 ],\n         [0.5529412 ],\n         [0.5529412 ]],\n\n        [[0.7568628 ],\n         [0.6627451 ],\n         [0.6627451 ],\n         ...,\n         [0.6431373 ],\n         [0.6431373 ],\n         [0.6509804 ]]]], dtype=float32), array([0., 3., 3., 4., 6., 6., 0., 5., 5., 2., 4., 2., 4., 0., 4., 6., 4.,\n       3., 5., 0., 3., 0., 0., 5., 2., 6., 6., 5., 2., 3., 3., 2., 5., 3.,\n       3., 3., 3., 0., 4., 3., 6., 5., 0., 3., 2., 4., 2., 3., 4., 2., 0.,\n       3., 6., 0., 4., 0., 3., 6., 6., 3., 3., 0., 3., 3.], dtype=float32)).\nTraceback (most recent call last):\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 204, in generator_py_func\n    flattened_values = nest.flatten_up_to(output_types, values)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 237, in flatten_up_to\n    return nest_util.flatten_up_to(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1541, in flatten_up_to\n    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1570, in _tf_data_flatten_up_to\n    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1427, in _tf_data_assert_shallow_structure\n    raise ValueError(\n\nValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\choud\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 206, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32, tf.float32), but the yielded element was (array([[[[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6745098 ],\n         [0.6784314 ],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6784314 ],\n         [0.67058825],\n         [0.6627451 ]],\n\n        [[1.        ],\n         [1.        ],\n         [1.        ],\n         ...,\n         [0.6666667 ],\n         [0.65882355],\n         [0.6745098 ]],\n\n        ...,\n\n        [[0.43921572],\n         [0.4431373 ],\n         [0.46274513],\n         ...,\n         [0.8117648 ],\n         [0.8196079 ],\n         [0.8117648 ]],\n\n        [[0.43921572],\n         [0.45098042],\n         [0.45098042],\n         ...,\n         [0.80392164],\n         [0.8078432 ],\n         [0.81568635]],\n\n        [[0.4431373 ],\n         [0.46274513],\n         [0.43921572],\n         ...,\n         [0.78823537],\n         [0.79215693],\n         [0.7960785 ]]],\n\n\n       [[[0.29803923],\n         [0.29803923],\n         [0.3019608 ],\n         ...,\n         [0.854902  ],\n         [0.83921576],\n         [0.82745105]],\n\n        [[0.32156864],\n         [0.30588236],\n         [0.29803923],\n         ...,\n         [0.85098046],\n         [0.8352942 ],\n         [0.8196079 ]],\n\n        [[0.34901962],\n         [0.34901962],\n         [0.34509805],\n         ...,\n         [0.8431373 ],\n         [0.82745105],\n         [0.81568635]],\n\n        ...,\n\n        [[0.3137255 ],\n         [0.18431373],\n         [0.13725491],\n         ...,\n         [0.09411766],\n         [0.10196079],\n         [0.19607845]],\n\n        [[0.25490198],\n         [0.16470589],\n         [0.13333334],\n         ...,\n         [0.12156864],\n         [0.13333334],\n         [0.30980393]],\n\n        [[0.18823531],\n         [0.14117648],\n         [0.14117648],\n         ...,\n         [0.15294118],\n         [0.23137257],\n         [0.38431376]]],\n\n\n       [[[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        [[0.2392157 ],\n         [0.23529413],\n         [0.24705884],\n         ...,\n         [0.40000004],\n         [0.40000004],\n         [0.40000004]],\n\n        ...,\n\n        [[0.22352943],\n         [0.19607845],\n         [0.3137255 ],\n         ...,\n         [0.28235295],\n         [0.28235295],\n         [0.28235295]],\n\n        [[0.21960786],\n         [0.20784315],\n         [0.20784315],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]],\n\n        [[0.20784315],\n         [0.227451  ],\n         [0.19215688],\n         ...,\n         [0.3803922 ],\n         [0.3803922 ],\n         [0.3803922 ]]],\n\n\n       ...,\n\n\n       [[[0.35686275],\n         [0.35686275],\n         [0.35686275],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.35686275],\n         [0.35686275],\n         [0.36078432],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        [[0.36078432],\n         [0.35686275],\n         [0.3529412 ],\n         ...,\n         [0.4666667 ],\n         [0.4666667 ],\n         [0.4666667 ]],\n\n        ...,\n\n        [[0.54509807],\n         [0.4666667 ],\n         [0.5058824 ],\n         ...,\n         [0.8352942 ],\n         [0.82745105],\n         [0.81568635]],\n\n        [[0.5254902 ],\n         [0.4784314 ],\n         [0.5176471 ],\n         ...,\n         [0.8196079 ],\n         [0.82745105],\n         [0.8352942 ]],\n\n        [[0.5019608 ],\n         [0.49411768],\n         [0.5176471 ],\n         ...,\n         [0.8078432 ],\n         [0.8078432 ],\n         [0.8078432 ]]],\n\n\n       [[[0.01568628],\n         [0.01568628],\n         [0.03137255],\n         ...,\n         [0.16862746],\n         [0.0627451 ],\n         [0.03529412]],\n\n        [[0.01568628],\n         [0.01960784],\n         [0.03137255],\n         ...,\n         [0.13333334],\n         [0.04313726],\n         [0.04313726]],\n\n        [[0.01568628],\n         [0.02352941],\n         [0.03137255],\n         ...,\n         [0.1137255 ],\n         [0.03137255],\n         [0.0509804 ]],\n\n        ...,\n\n        [[0.47450984],\n         [0.49411768],\n         [0.49411768],\n         ...,\n         [0.06666667],\n         [0.06666667],\n         [0.07058824]],\n\n        [[0.4901961 ],\n         [0.5137255 ],\n         [0.5372549 ],\n         ...,\n         [0.02745098],\n         [0.03137255],\n         [0.03529412]],\n\n        [[0.5254902 ],\n         [0.5411765 ],\n         [0.56078434],\n         ...,\n         [0.00392157],\n         [0.00392157],\n         [0.00392157]]],\n\n\n       [[[0.6117647 ],\n         [0.6117647 ],\n         [0.40784317],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        [[0.60784316],\n         [0.61960787],\n         [0.47058827],\n         ...,\n         [0.654902  ],\n         [0.654902  ],\n         [0.654902  ]],\n\n        [[0.6       ],\n         [0.5921569 ],\n         [0.4039216 ],\n         ...,\n         [0.6509804 ],\n         [0.6509804 ],\n         [0.6509804 ]],\n\n        ...,\n\n        [[0.7411765 ],\n         [0.6392157 ],\n         [0.6117647 ],\n         ...,\n         [0.4431373 ],\n         [0.45098042],\n         [0.45098042]],\n\n        [[0.74509805],\n         [0.6392157 ],\n         [0.63529414],\n         ...,\n         [0.5529412 ],\n         [0.5529412 ],\n         [0.5529412 ]],\n\n        [[0.7568628 ],\n         [0.6627451 ],\n         [0.6627451 ],\n         ...,\n         [0.6431373 ],\n         [0.6431373 ],\n         [0.6509804 ]]]], dtype=float32), array([0., 3., 3., 4., 6., 6., 0., 5., 5., 2., 4., 2., 4., 0., 4., 6., 4.,\n       3., 5., 0., 3., 0., 0., 5., 2., 6., 6., 5., 2., 3., 3., 2., 5., 3.,\n       3., 3., 3., 0., 4., 3., 6., 5., 0., 3., 2., 4., 2., 3., 4., 2., 0.,\n       3., 6., 0., 4., 0., 3., 6., 6., 3., 3., 0., 3., 3.], dtype=float32)).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_7341]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
